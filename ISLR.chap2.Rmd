---
title: "ISLR.chapter3"
author: "Shahina Rahman"
date: "7/3/2021"
output: pdf_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Lab:  Simple Linear Regression 
1. Boston has 506 neighborhoods around Boston. We will seek to predict `medv` using 13 predictors. Start with simple linear regression.  

```{r 1}
library(MASS)
attach(Boston)
lm.fit <- lm(medv ~ lstat, data = Boston)
summary(lm.fit)
```

Coeficients

```{r 2}
names(lm.fit)
coef(lm.fit)
```

confidence interval 

```{r 3}
confint(lm.fit, level = 0.95)
```

prediction 

```{r 4}
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval = "confidence")
```

Prediction Interval 

```{r 5}
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval = "prediction")
```

Both the confidence and prediction interval has the same mid point. 

## However, the prediction interval is much wider than the confidence interval is. Why ? 

because they incorporate both the error in the estimate for f(X) (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).
We use a confidence interval to quantify the uncertainty surrounding the `average sales over a large number of cities`. A prediction interval can be used to quantify the uncertainty surrounding sales for `a particular city`. 


## Including Plots

You can also embed plots, for example:

```{r abline(lm.fit)}
plot(lstat, medv, pch = 3, col = 'gray')
abline(lm.fit, col = 'blue', lw = 3)
```

abline(a,b) means intercept = a and slope = b. 


## Diagnostic Plots 

```{r diagnostic}
par(mfrow = c(2,2))
plot(lm.fit)
```


```{r diagnostic1}
plot(predict(lm.fit), residuals(lm.fit), pch = 20, col = "gray")
plot(predict(lm.fit), rstudent(lm.fit), pch = 20, col = "gray")
```


# Insights into pvalue and test-statistics 

We are testing:  Under the assumption of $y = \beta_0 + \beta X + error$, if $\beta = 0$ or not.
The test statistics is : 
$t = \frac{\hat{\beta} - 0}{\hat{s.e(\hat{\beta})}}$ . 
If there is no linear relationship between Y and X, then the test statistics will follow a t distribution with $n - 2$ degrees of freedom.
t distribution has a bell shaped. For a large n, the t distribution will behave similar to normal. pvalue is the chance of observing a high abs value of t under the null. If it is very high, then there is no evidence. But if it is too small then there is a significant evidence. 

Small pvalue means: it is unlikely to observe such observations under null assumption. 


# Assessing the Accuracy of the Linear Model 


training error/ testing error. 

The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the R-squared statistic.

Because of the presence of the error term $\epsilon$, even if we knew the true regression line (i.e. even if coefficients were known), we would not be able to perfectly predict $Y$ from $X$. 

usually we assume that $var(\epsilon) = \sigma^2$, a constant quantity for linear regression. 

RSE is the estimate of $\hat{\sigma}$. So if you look at the model, 

$y = \alpha + \beta X + error$. Then RSE is the estimate of the deviation of the response from its expected regression line. It is computed using 
$\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i - \hat{y_i})} = \sqrt{\frac{1}{n-2} RSS}$. 

whereas, $R-square$ is unit free and explains the proportion of variance explained by the data itself. 

$1 - \frac{RSS}{TSS}$.





# Lab : Multiple Linear Regression 
Instead of doing multiple t test, you do a F test on $H_0: \beta_0 = beta_1 = \cdots = \beta_p = 0$. The approach of using an F-statistic to test for any association between the predictors and the response works when p is relatively small, and certainly small compared to n.

### Cautious: It turns out that R2, which is computed on training data will always increase when more variable are added to the model, even if those variables are only weakly associated with the response. 

This is due to the fact that adding another variable to the least squares equations must allow us to fit the training data (though not necessarily the testing data) more accurately.


```{r m1}
lm.fit = lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

We don't want to type all the variables so 


```{r m2}
lm.fit = lm(medv ~ ., data = Boston)
res = summary(lm.fit)
res$sigma
res$r.squared
confint(res, level = 0.95, interval = )
```

Suppose we want to remove the age variables so 

```{r m3}
lm.fit = lm(medv ~ .-age, data = Boston)
res = summary(lm.fit)
res
#res$sigma
#res$r.squared
```


## Interaction term 

icecream sales gets “credit” for the effect of temperature on shark attacks. So correlation matrix is very important to look at. 

```{r m4}
lm.fit = lm(medv ~ lstat * age, data = Boston)
res = summary(lm.fit)
res
#res$sigma
#res$r.squared    
```


## The hierarchy principle : The principle states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.

lstat, age and the interaction term are significant in predicting the medv. 

